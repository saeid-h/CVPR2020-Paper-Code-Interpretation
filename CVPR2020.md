# Download the latest information and papers CVPR2020 Papers/Codes/Project/PaperReading／Demos/Live sharing／Paper sharing meeting, etc.）


Official website link：http://cvpr2020.thecvf.com/<br>
time：Seattle, Washington，June 14-June 19, 2020 <br>
Paper acceptance time：February 24, 2020<br>

related question：
* [How to evaluate the phenomenon that the number of CVPR submissions in the Computer Vision Summit will exceed 10,000 in 2020?](https://www.zhihu.com/question/356099725/)<br>
* [How to evaluate the acceptance results of CVPR 2020? What are the highlights?](https://www.zhihu.com/question/372070853)<br><br>

# List
[1. CVPR2020 receiving papers (continuously updated)）](#100)<br>
[2. CVPR2020 Oral (continuously updated)](#101)<br>
[3. CVPR2020 paper interpretation](#102)<br>
[4. To do list](#103)<br>
[5. Related works](#104)<br>


<br><br>

<a name="100"/>

# 1.CVPR2020 receiving papers（Continually updated）<br>




<br>

### table of Contents<br>
[1. Target Detection](#1)<br>
[2. Face recognition](#3)<br>
[3. Target Tracking](#4)<br>
[4. 3D point cloud / 3D reconstruction / 3D detection / 3D segmentation / depth estimation](#5)<br>
[5. Image Identification](#18)<br>
[6. Image Processing](#6)<br>
[7. Image classification](#7)<br>
[8. Image segmentation](#2)<br>
[9. Attitude estimation / motion recognition](#8)<br>
[10. Video analysis](#9)<br>
[11. OCR](#10)<br>
[12. GAN](#11)<br>
[13. Small sample / zero sample](#12)<br>
[14. Weak Supervision / Unsupervised / Self Supervision](#13)<br>
[15. Pedestrian tracking / pedestrian detection/ReID](#14)<br>
[16. Neural network / model acceleration / model compression](#15)<br>
[17. Super resolution](#16)<br>
[18. Visual Commons / Data Set / Other](#17)<br>


<br><br>

<a name="1"/>

### Target Detection

1. Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection<br>
Paper：https://arxiv.org/abs/1912.02424   <br>
Code：https://github.com/sfzhang15/ATSS<br><br>

2. Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector<br>
Paper url：https://arxiv.org/abs/1908.01998<br><br>

3. AugFPN: Improving Multi-scale Feature Learning for Object Detection<br>
Paper：https://arxiv.org/abs/1912.05384<br><br>

4. Hit-Detector: Hierarchical Trinity Architecture Search for Object Detection<br>
Pape：https://arxiv.org/abs/2003.11818<br>
Code：https://github.com/ggjy/HitDet.pytorch<br><br>

5. Multi-task Collaborative Network for Joint Referring Expression Comprehension and Segmentation<br>
Paper：https://arxiv.org/abs/2003.08813<br><br>

6. CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection<br>
Paper：https://arxiv.org/abs/2003.09119<br>
Code：https://github.com/KiveeDong/CentripetalNet<br><br>


<br><br>

<a name="3"/>

### Face recognition

1. Towards Universal Representation Learning for Deep Face Recognition<br>
Paper：https://arxiv.org/abs/2002.11841<br><br>

2. Suppressing Uncertainties for Large-Scale Facial Expression Recognition   <br>    
Paper：https://arxiv.org/abs/2002.10392<br>
Code：https://github.com/kaiwang960112/Self-Cure-Network<br><br>

3. Face X-ray for More General Face Forgery Detection<br>
Paper：https://arxiv.org/pdf/1912.13458.pdf<br><br>

4. Pose Agnostic Cross-spectral Hallucination via Disentangling Independent Factors<br>
Paper：https://arxiv.org/abs/1909.04365<br><br>

5. Deep Spatial Gradient and Temporal Depth Learning for Face Anti-spoofing<br>
Paper：https://arxiv.org/abs/2003.08061<br>
Code：https://github.com/clks-wzz/FAS-SGTD<br><br>

6. Learning Meta Face Recognition in Unseen Domains<br>
Paper：https://arxiv.org/abs/2003.07733<br>
Code：https://github.com/cleardusk/MFR<br><br>


<br><br>

<a name="4"/>

### Target Tracking

1. ROAM: Recurrently Optimizing Tracking Model<br>
Paper：https://arxiv.org/abs/1907.12006 <br><br>

<br><br>

<a name="5"/>

### 3D point cloud / 3D reconstruction / 3D detection / 3D segmentation / depth estimation

* 3D point cloud & reconstruction

1. PF-Net: Point Fractal Network for 3D Point Cloud Completion<br>
Paper：https://arxiv.org/abs/2003.00410<br><br>

2. PointAugment: an Auto-Augmentation Framework for Point Cloud Classification<br>
Paper：https://arxiv.org/abs/2002.10876<br>
Code：https://github.com/liruihui/PointAugment/<br><br>

3. Learning multiview 3D point cloud registration<br>
Paper：https://arxiv.org/abs/2001.05119<br><br>

4. C-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds<br>
Paper：https://arxiv.org/abs/1912.07009<br><br>

5. RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds<br>
Paper：https://arxiv.org/abs/1911.11236 <br><br>


6. Total3DUnderstanding: Joint Layout, Object Pose and Mesh Reconstruction for Indoor Scenes from a Single Image<br>
Paper：https://arxiv.org/abs/2002.12212<br><br>

7. Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion<br>
Paper：https://arxiv.org/abs/2003.01456<br><br>

8. In Perfect Shape: Certifiably Optimal 3D Shape Reconstruction from 2D Landmarks<br>
Paper：https://arxiv.org/pdf/1911.11924.pdf<br><br>

9. Attentive Context Normalization for Robust Permutation-Equivariant Learning<br>
Paper：https://arxiv.org/abs/1907.02545	Weiwei Sun, Wei Jiang, Eduard Trulls, Andrea Tagliasacchi, Kwang Moo Yi<br><br>

10. PQ-NET: A Generative Part Seq2Seq Network for 3D Shapes	<br>
Paper：https://arxiv.org/abs/1911.10949	<br><br>

11. SG-NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans<br>
Paper：https://arxiv.org/abs/1912.00036<br><br>

12. Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching<br>
Paper：https://arxiv.org/abs/1912.06378<br>
Code：https://github.com/alibaba/cascade-stereo<br><br>

13. Unsupervised Learning of Intrinsic Structural Representation Points<br>
Paper：https://arxiv.org/abs/2003.01661<br>
Code：https://github.com/NolenChen/3DStructurePoints<br><br>

* Three-dimensional reconstruction
1. Leveraging 2D Data to Learn Textured 3D Mesh Generation <br>	
Paper：https://arxiv.org/abs/2004.04180<br><br>

2. ARCH: Animatable Reconstruction of Clothed Humans<br>	
Paper：https://arxiv.org/abs/2004.04572<br><br>

3. Learning 3D Semantic Scene Graphs from 3D Indoor Reconstructions<br>	
Paper：https://arxiv.org/abs/2004.03967<br><br>

<br><br>

<a name="18"/>

### Image Identification

* Image feature matching
1. Reinforced Feature Points: Optimizing Feature Detection and Description for a High-Level Task <br>
Paper：https://arxiv.org/abs/1912.00623<br><br>

2. Correspondence Networks with Adaptive Neighbourhood Consensus <br>
Paper：https://arxiv.org/abs/2003.12059<br><br>

* Image subtitles
10. Normalized and Geometry-Aware Self-Attention Network for Image Captioning <br>
Paper：https://arxiv.org/abs/2003.08897<br><br>

<br><br>

<a name="6"/>

### Image Processing

1. Learning to Shade Hand-drawn Sketches<br>
Paper：https://arxiv.org/abs/2002.11812<br><br>

2. Single Image Reflection Removal through Cascaded Refinement<br>
Paper：https://arxiv.org/abs/1911.06634<br><br>

3. Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data<br>
Paper：https://arxiv.org/abs/2002.11297<br><br>

4. Deep Image Harmonization via Domain Verification<br>
Paper：https://arxiv.org/abs/1911.13239<br>
Code：https://github.com/bcmi/Image_Harmonization_Datasets<br><br>

5. RoutedFusion: Learning Real-time Depth Map Fusion<br>
Paper：https://arxiv.org/pdf/2001.04388.pdf <br><br>

6. Neural Contours: Learning to Draw Lines from 3D Shapes<br>
Paper：https://arxiv.org/abs/2003.10333<br><br>

7. Towards Photo-Realistic Virtual Try-On by Adaptively Generating鈫Preserving Image Content<br>
Paper：https://arxiv.org/abs/2003.05863<br><br>



<br><br>

<a name="7"/>

### Image classification

1. Self-training with Noisy Student improves ImageNet classification<br>
Paper：https://arxiv.org/abs/1911.04252<br><br>

2. Image Matching across Wide Baselines: From Paper to Practice<br>
Paper：https://arxiv.org/abs/2003.01587<br><br>

3. Towards Robust Image Classification Using Sequential Attention Models<br>
Paper：https://arxiv.org/abs/1912.02184<br><br>

4. Learning in the Frequency Domain	<br>
Paper：https://arxiv.org/abs/2002.12416<br><br>

5. Learning from Web Data with Memory Module	<br>
Paper：https://arxiv.org/abs/1906.12028<br><br>

6. Making Better Mistakes: Leveraging Class Hierarchies with Deep Networks	<br>
Paper：https://arxiv.org/abs/1912.09393 <br><br>


<br><br>

<a name="2"/>

### Image segmentation

1. Semi-Supervised Semantic Image Segmentation with Self-correcting Networks<br>
Paper：https://arxiv.org/abs/1811.07073<br><br>

2. Deep Snake for Real-Time Instance Segmentation<br>
Paper：https://arxiv.org/abs/2001.01629<br><br>

3. CenterMask : Real-Time Anchor-Free Instance Segmentation<br>
Paper：https://arxiv.org/abs/1911.06667<br>
Code：https://github.com/youngwanLEE/CenterMask<br><br>

4. SketchGCN: Semantic Sketch Segmentation with Graph Convolutional Networks<br>
Paper：https://arxiv.org/abs/2003.00678<br><br>

5. PolarMask: Single Shot Instance Segmentation with Polar Representation<br>
Paper：https://arxiv.org/abs/1909.13226<br>
Code：https://github.com/xieenze/PolarMask<br><br>


6. xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation<br>
Paper：https://arxiv.org/abs/1911.12676<br><br>

7. BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation<br>
Paper：https://arxiv.org/abs/2001.00309<br><br>

8. Enhancing Generic Segmentation with Learned Region Representations<br>
Paper：https://arxiv.org/abs/1911.08564<br><br>

<br><br>

<a name="8"/>

### Attitude estimation / motion recognition

1. VIBE: Video Inference for Human Body Pose and Shape Estimation<br>
Paper：https://arxiv.org/abs/1912.05656  <br> 
Code：https://github.com/mkocabas/VIBE<br><br>

2. Distribution-Aware Coordinate Representation for Human Pose Estimation<br>
Paper：https://arxiv.org/abs/1910.06278   <br>
Code：https://github.com/ilovepose/DarkPose<br><br>

3. 4D Association Graph for Realtime Multi-person Motion Capture Using Multiple Video Cameras<br>
Paper：https://arxiv.org/abs/2002.12625<br><br>

4. Optimal least-squares solution to the hand-eye calibration problem<br>
Paper：https://arxiv.org/abs/2002.10838<br><br>

5. D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry<br>
Paper：https://arxiv.org/abs/2003.01060<br><br>

6. Multi-Modal Domain Adaptation for Fine-Grained Action Recognition<br>
Paper：https://arxiv.org/abs/2001.09691<br><br>

7. Distribution Aware Coordinate Representation for Human Pose Estimation<br>
Paper：https://arxiv.org/abs/1910.06278<br><br>

8. The Devil is in the Details: Delving into Unbiased Data Processing for Human Pose Estimation<br>
Paper：https://arxiv.org/abs/1911.07524<br><br>

9. PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose Estimation<br>
Paper：https://arxiv.org/abs/1911.04231<br><br>

10. Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation<br>
Paper：https://arxiv.org/abs/2003.02824<br><br>

11. G2L-Net: Global to Local Network for Real-time 6D Pose Estimation with Embedding Vector Features<br>
Paper：https://arxiv.org/abs/2003.11089<br><br>

12. Deep Image Spatial Transformation for Person Image Generation<br>
Paper：https://arxiv.org/abs/2003.00696<br>
Code：https://github.com/RenYurui/ Global-Flow-Local-Attention<br><br>


<br><br>

<a name="9"/>

### Video analysis

1. Rethinking Zero-shot Video Classification: End-to-end Training for Realistic Applications<br>
Paper：https://arxiv.org/abs/2003.01455   <br>
Code：https://github.com/bbrattoli/ZeroShotVideoClassification<br><br>

2. Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs<br>
Paper：https://arxiv.org/abs/2003.00387<br><br>

3. Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning<br>
Paper：https://arxiv.org/abs/2003.00392<br><br>

4. Object Relational Graph with Teacher-Recommended Learning for Video Captioning<br>
Paper：https://arxiv.org/abs/2002.11566<br><br>

5. Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution<br>
Paper：https://arxiv.org/abs/2002.11616<br><br>

6. Blurry Video Frame Interpolation<br>
Paper：https://arxiv.org/abs/2002.12259<br><br>

7. Hierarchical Conditional Relation Networks for Video Question Answering<br>
Paper：https://arxiv.org/abs/2002.10698   <br><br>

8. Action Modifiers:Learning from Adverbs in Instructional Video<br>
Paper：https://arxiv.org/abs/1912.06617     <br><br>

9. Visual Grounding in Video for Unsupervised Word Translation<br>
Paper：https://arxiv.org/abs/2003.05078<br>
Code：https://github.com/gsig/visual-grounding<br><br>

10. MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask（视频分析-光流估计）<br>
Paper：https://arxiv.org/abs/2003.10955<br>
Code：https://github.com/microsoft/MaskFlownet<br><br>

11. Use the Force, Luke! Learning to Predict Physical Forces by Simulating Effects（视频预测）<br>
Paper：https://arxiv.org/abs/2003.12045<br>
Code：https://ehsanik.github.io/forcecvpr2020<br><br>


<br><br>

<a name="10"/>

### OCR

1. ABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network<br>
Paper：https://arxiv.org/abs/2002.10200<br>
Code：https://github.com/Yuliang-Liu/bezier_curve_text_spotting,https://github.com/aim-uofa/adet<br><br>

2. Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA<br>
Paper：https://arxiv.org/abs/1911.06258<br><br>

<br><br>

<a name="11"/>

### GAN

1. Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models<br>
Paper：https://arxiv.org/abs/1911.12287<br>
Code：https://github.com/giannisdaras/ylg<br><br>

2. MSG-GAN: Multi-Scale Gradient GAN for Stable Image Synthesis<br>
Paper：https://arxiv.org/abs/1903.06048<br><br>

3. Robust Design of Deep Neural Networks against Adversarial Attacks based on Lyapunov Theory<br>
Paper：https://arxiv.org/abs/1911.04636<br><br>

4. PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer<br>
Paper：https://arxiv.org/abs/1909.06956<br><br>


<br><br>

<a name="12"/>

### Small sample / zero sample

1. Improved Few-Shot Visual Classification<br>
Paper：https://arxiv.org/pdf/1912.03432.pdf <br><br>

2. Meta-Transfer Learning for Zero-Shot Super-Resolution<br>
Paper：https://arxiv.org/abs/2002.12213<br><br>

3. Instance Credibility Inference for Few-Shot Learning<br>
Paper：https://arxiv.org/abs/2003.11853<br>
Code：https://github.com/Yikai-Wang/ICI-FSL<br><br>

<br><br>

<a name="13"/>

### Weak Supervision / Unsupervised / Self Supervision

1. Rethinking the Route Towards Weakly Supervised Object Localization<br>
Paper：https://arxiv.org/abs/2002.11359<br><br>

2. NestedVAE: Isolating Common Factors via Weak Supervision<br>
Paper：https://arxiv.org/abs/2002.11576 <br><br>

3. Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation<br>
Paper：https://arxiv.org/abs/1911.07450<br><br>

4. Disentangling Physical Dynamics from Unknown Factors for Unsupervised Video Prediction<br>
Paper：https://arxiv.org/abs/2003.01460<br><br>

5. ClusterFit: Improving Generalization of Visual Representations<br>
Paper：https://arxiv.org/abs/1912.03330<br><br>

6. Auto-Encoding Twin-Bottleneck Hashing<br>
Paper：https://arxiv.org/abs/2002.11930<br><br>

7. Learning Representations by Predicting Bags of Visual Words<br>
Paper：https://arxiv.org/abs/2002.12247<br><br>

8. A Characteristic Function Approach to Deep Implicit Generative Modeling<br>
Paper：https://arxiv.org/abs/1909.07425<br><br>

9. Unsupervised Learning of Intrinsic Structural Representation Points<br>
Paper：https://arxiv.org/abs/2003.01661<br>
Code：https://github.com/NolenChen/3DStructurePoints<br><br>


<br><br>

<a name="14"/>

### Pedestrian tracking / pedestrian detection / ReID
1. Cross-modality Person re-identification with Shared-Specific Feature Transfer	<br>
Paper：https://arxiv.org/abs/2002.12489 <br><br>

2. Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction	<br>
Paper：https://arxiv.org/abs/2002.11927<br><br>

3. The Garden of Forking Paths: Towards Multi-Future Trajectory Prediction	<br>
Paper：https://arxiv.org/abs/1912.06445<br><br>

<br><br>

<a name="15"/>

### Neural network / model compression / model acceleration

1. GhostNet: More Features from Cheap Operations<br>
Paper：https://arxiv.org/abs/1911.11907<br>
Code：https://github.com/iamhankai/ghostnet<br><br>

2. Watch your Up-Convolution: CNN Based Generative Deep Neural Networks are Failing to Reproduce Spectral <br>
Paper：https://arxiv.org/abs/2003.01826<br><br>

3. GPU-Accelerated Mobile Multi-view Style Transfer<br>
Paper：https://arxiv.org/abs/2003.00706<br><br>

4. Bundle Adjustment on a Graph Processor		<br>
Paper：https://arxiv.org/abs/2003.03134		<br>
Code：https://github.com/joeaortiz/gbp<br><br>

5. Watch your Up-Convolution: CNN Based Generative Deep Neural Networks are Failing to Reproduce Spectral 	<br>
Paper：https://arxiv.org/abs/2003.01826	<br><br>

6. Holistically-Attracted Wireframe Parsing	<br>
Paper：https://arxiv.org/abs/2003.01663	<br><br>

7. AdderNet: Do We Really Need Multiplications in Deep Learning? 	<br>
Paper：https://arxiv.org/abs/1912.13200 	<br><br>

8. CARS: Contunuous Evolution for Efficient Neural Architecture Search	<br>
Paper：https://arxiv.org/abs/1909.04977	 	<br>
Code：https://github.com/huawei-noah/CARS<br><br>

9. Π-nets: Deep Polynomial Neural Networksv<br>
Paper：https://arxiv.org/abs/2003.03828<br><br>

10. Explaining Knowledge Distillation by Quantifying the Knowledge<br>
Paper：https://arxiv.org/abs/2003.03622<br><br>

<br><br>

<a name="16"/>

### Super resolution

1. Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution	<br>
Paper：https://arxiv.org/abs/2002.11616<br><br>

2. Closed-loop Matters: Dual Regression Networks for Single Image Super-Resolution <br>
Paper：https://arxiv.org/abs/2003.07018<br>
Code：https://github.com/guoyongcs/DRN<br><br>

<br><br>

<a name="17"/>

### Visual Commons / Other

1. Visual Commonsense R-CNN<br>
Paper：https://arxiv.org/abs/2002.12204<br>
Code：https://github.com/Wangt-CN/VC-R-CNN<br><br>

2. Scalable Uncertainty for Computer Vision with Functional Variational Inference	<br>
Paper：https://arxiv.org/abs/2003.03396<br><br>

3. Deep Representation Learning on Long-tailed Data: A Learnable Embedding Augmentation Perspective	<br>
Paper：https://arxiv.org/abs/2002.10826<br><br>

4. Representations, Metrics and Statistics For Shape Analysis of Elastic Graphs	<br>
Paper：https://arxiv.org/abs/2003.00287	<br><br>
				
5. Filter Grafting for Deep Neural Networks	<br>
Paper：https://arxiv.org/abs/2001.05868<br>
Code：https://github.com/fxmeng/filter-grafting.git<br><br>

6. 12-in-1: Multi-Task Vision and Language Representation Learning<br>
Paper：https://arxiv.org/abs/1912.02315 	<br><br>
				
7. Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training	<br>
Paper：https://arxiv.org/abs/2002.10638		<br>
Code：https://github.com/weituo12321/PREVALENT<br><br>

8. Unbiased Scene Graph Generation from Biased Training	<br>
Paper：https://arxiv.org/abs/2002.11949 <br><br>

9.Towards Visually Explaining Variational Autoencoders<br>
Paper：https://arxiv.org/abs/1911.07389<br><br>

10. BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition<br>
Paper：http://www.weixiushen.com/publication/cvpr20_BBN.pdf<br>
Code：https://github.com/Megvii-Nanjing/BBN<br><br>

11. High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks<br>
Paper：https://arxiv.org/abs/1905.13545<br><br>

12. SAM: The Sensitivity of Attribution Methods to Hyperparameters<br>
Paper：http://s.anhnguyen.me/sam\_cvpr2020.pdf<br>
Code：https://github.com/anguyen8/sam<br><br>

13. Π− nets: Deep Polynomial Neural Networks<br>
Paper：https://arxiv.org/abs/2003.03828<br><br>

14. Towards Backward-Compatible Representation Learning<br>
Paper：https://arxiv.org/abs/2003.11942<br><br>

15. On Translation Invariance in CNNs: Convolutional Layers can Exploit Absolute Spatial Location<br>
Paper：https://arxiv.org/abs/2003.07064<br><br>

16. KeypointNet: A Large-scale 3D Keypoint Dataset Aggregated from Numerous Human Annotations（数据集）<br>
Paper：https://arxiv.org/abs/2002.12687<br><br>


<br><br>

<a name="101"/>

# 2.CVPR2020 Oral（Continually updated）<br>
[1. PolarMask: Single Shot Instance Segmentation with Polar Representation](https://arxiv.org/abs/1909.13226)<br>
Code：https://github.com/xieenze/PolarMask <br><br>

[2. Unbiased Scene Graph Generation from Biased Training](https://arxiv.org/abs/2002.11949) <br>
Code：https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch <br><br>

[3. Learning to Shade Hand-drawn Sketches](https://arxiv.org/abs/2002.11812) <br>
Code：https://github.com/qyzdao/ShadeSketch <br><br>

[4. SAM: The Sensitivity of Attribution Methods to Hyperparameters](http://s.anhnguyen.me/sam_cvpr2020.pdf)<br>
Code：https://github.com/anguyen8/sam<br><br>

[5. High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks](https://arxiv.org/abs/1905.13545)<br><br>

[6. Reinforced Feature Points: Optimizing Feature Detection and Description for a High-Level Task](https://arxiv.org/abs/1912.00623) <br><br>

[7. RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds](https://arxiv.org/abs/1911.11236)<br><br>

[8. AdderNet: Do We Really Need Multiplications in Deep Learning? ](https://arxiv.org/abs/1912.13200 )<br><br>

[9. Multi-Modal Domain Adaptation for Fine-Grained Action Recognition](https://arxiv.org/abs/2001.09691 )<br><br>

[10. Multi-task Collaborative Network for Joint Referring Expression Comprehension and Segmentation](https://arxiv.org/abs/2003.08813)<br><br>

[11. Deep Spatial Gradient and Temporal Depth Learning for Face Anti-spoofing](https://arxiv.org/abs/2003.08061)<br>
https://github.com/clks-wzz/FAS-SGTD<br><br>

[12. Learning Meta Face Recognition in Unseen Domains](https://arxiv.org/abs/2003.07733)<br>
https://github.com/cleardusk/MFR<br><br>

[13. Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching](https://arxiv.org/abs/1912.06378)<br>
https://github.com/alibaba/cascade-stereo<br><br>

[14. BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition](http://www.weixiushen.com/publication/cvpr20_BBN.pdf)<br>
https://github.com/Megvii-Nanjing/BBN<br><br>

[15. High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks](https://arxiv.org/abs/1905.13545)<br><br>

[16. SAM: The Sensitivity of Attribution Methods to Hyperparameters](http://s.anhnguyen.me/sam\_cvpr2020.pdf)<br>
https://github.com/anguyen8/sam<br><br>

[17. Towards Backward-Compatible Representation Learning](https://arxiv.org/abs/2003.11942)<br><br>

[18. MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask](https://arxiv.org/abs/2003.10955)<br>
https://github.com/microsoft/MaskFlownet<br><br>

[19. Use the Force, Luke! Learning to Predict Physical Forces by Simulating Effects](https://arxiv.org/abs/2003.12045)<br>
https://ehsanik.github.io/forcecvpr2020<br><br>

[20. StyleRig: Rigging StyleGAN for 3D Control over Portrait Images](https://arxiv.org/abs/2004.00121)<br><br>

[21. Conditional Channel Gated Networks for Task-Aware Continual Learning](https://arxiv.org/abs/2004.00070)

[22. BANet: Bidirectional Aggregation Network with Occlusion Handling for Panoptic Segmentation](https://arxiv.org/abs/2003.14031)<br><br>

[23. TITAN: Future Forecast using Action Priors](https://arxiv.org/abs/2003.13886)<br><br>

[24. Learning Interactions and Relationships between Movie Characters](https://arxiv.org/abs/2003.13158)<br><br>

[25. GPS-Net: Graph Property Sensing Network for Scene Graph Generation](https://arxiv.org/abs/2003.12962)<br>
https://github.com/taksau/GPS-Net<br><br>

[26. A Physics-based Noise Formation Model for Extreme Low-light Raw Denoising](https://arxiv.org/abs/2003.12751)<br>
https://github.com/Vandermode/NoiseModel<br><br>

[27. Controllable Person Image Synthesis with Attribute-Decomposed GAN](https://arxiv.org/abs/2003.12267)<br>
https://menyifang.github.io/projects/ADGAN/ADGAN.html<br><br>

[28. Towards Discriminability and Diversity: Batch Nuclear-norm Maximization under Label Insufficient Situations](https://arxiv.org/abs/2003.12237)<br><br>

[29. Learning to Optimize Non-Rigid Tracking](https://arxiv.org/abs/2003.12230)<br><br>

[30. Self-Supervised Scene De-occlusion](https://arxiv.org/abs/2004.02788)<br>
https://xiaohangzhan.github.io/projects/deocclusion/<br><br>

[31. Robust 3D Self-portraits in Seconds](https://arxiv.org/abs/2004.02460)<br><br>

[32. Steering Self-Supervised Feature Learning Beyond Local Pixel Statistics](https://arxiv.org/abs/2004.02331)<br><br>

[33. Light Field Spatial Super-resolution via Deep Combinatorial Geometry Embedding and Structural Consistency Regularization](https://arxiv.org/abs/2004.02215)<br><br>

[34. Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval](https://arxiv.org/abs/2004.01804)<br><br>

[35. Deep White-Balance Editing](https://arxiv.org/abs/2004.01354)<br><br>

[36. Tracking by Instance Detection: A Meta-Learning Approach](https://arxiv.org/abs/2004.00830)<br><br>



<br><br>

<a name="102"/>

# 3.CVPR2020 paper interpretation<br><br>


### [15.Unsupervised Learning of Visual Common Sense Features——A Probe into Causality](https://zhuanlan.zhihu.com/p/111306353)<br>
Now more and more researchers are beginning to pay attention to how to apply the cause and effect in statistics to deep learning to increase its robustness, interpretability and so on. But most of the work did not go into causal theory, and more borrowed some of these concepts (such as counterfactual counterfactuals). This paper aims to go further on this basis.<br>
Paper：https://arxiv.org/abs/2002.12204<br>
Code：https://github.com/Wangt-CN/VC-R-CNN<br><br>

### [14.CVPR2020 | The latest and most complete scene graph generation (SGG) open source framework, integrating the most complete metrics currently, has been open source](https://mp.weixin.qq.com/s/Nj6GjpRG8qG1ihhcoY9SwQ)<br>
Choose the popular framework facebookresearch / maskrcnn-benchmark of 2019 as the foundation, and build Scene-Graph-Benchmark.pytorch on the basis of it. This code is not only compatible with all detector models supported by maskrcnn-benchmark, but also benefits from facebookresearch's excellent code skills, which greatly increases the readability and operability of the SGG part.<br>
Paper：https://arxiv.org/abs/2002.11949<br>
Code：https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch<br><br>


### [13.CVPR2020 | Ignorance Research Institute proposes a monocular 6DoF pose estimation algorithm based on 3D key point voting network (open source)](https://mp.weixin.qq.com/s/c8rQYj5lSOtI1iza9e0Dpw)<br>
Paper：https://arxiv.org/abs/1911.04231<br>
Code：https://github.com/ethnhe/PVN3D.git<br>
The Defiance Research Institute proposes a 3D keypoint detection neural network based on Hough voting, called PVN3D, to learn the point-to-point offset to the 3D keypoint and vote for the 3D keypoint. The method based on 2D key points is advanced to 3D key points to make full use of the geometric constraint information of the rigid body, which greatly improves the accuracy of 6DoF estimation. Evaluation experiments were conducted on two public data sets, YCB-Video and LineMOD, and the results show that the method achieves the best current performance with a large advantage.
<br><br>


### [12.Cross-modal pedestrian re-identification: shared and specific feature transformation algorithm cm-SSFT](https://mp.weixin.qq.com/s/qPc71o2JeMDpDgRxDtp2BA)<br>
Paper：https://arxiv.org/abs/2002.12489<br>
Pay attention to infrared-RGB cross-modal pedestrian re-identification. Attempt to solve: In the past, most cross-modal pedestrian re-recognition algorithms generally only focused on shared feature learning, and rarely focused on Specific features. Because Specific feature does not exist in the opposite mode. For example, there is no color information in infrared pictures. Conversely, there will be no heat information in the color pictures. In fact, everyone who has done ReID knows that the reason why traditional ReID has high performance is largely due to some "overfitting" to these specific information. For example, clothes color has always been an important cue of traditional ReID. From this perspective, try to use specific features. The main idea is to use the neighbor information: given an infrared query. When searching for a color target, you can first find some simple color samples with high confidence (these samples are most likely to be positive samples of infrared query), and give the color specific characteristics of these color samples to infrared query. After doing this, infrared query samples can use this color information to search for more difficult color samples.<br><br>

### [11.RandLA-Net:New framework for 3D point cloud semantic segmentation of large scenes (open source)](https://mp.weixin.qq.com/s/xuLJ8m_ipGVBXVduA7Y0IA)<br>
Paper：https://arxiv.org/abs/1911.11236<br>
Code：https://github.com/QingyongHu/RandLA-Net<br>
A network structure based on simple and efficient random downsampling and local feature aggregation (RandLA-Net) is proposed. This method not only achieves very good results on large scene point cloud segmentation datasets such as Semantic3D and SemanticKITTI, but also has very high efficiency (e.g. is nearly 200 times faster than the graph-based method SPG).<br><br>


### [10.Tencent launches super-strong, small-sample target detection algorithm, and publicizes thousands of small-sample detection training sets FSOD](https://mp.weixin.qq.com/s/TRRsBGzMir0ttzjTdXwJCw)<br>
Paper：https://arxiv.org/abs/1908.01998<br>
A new low-sample target detection algorithm is proposed. The innovations include Attention-RPN, multi-relational detector and comparative training strategy. In addition, a small sample detection data set FSOD containing 1000 classes is constructed. Move directly to the new category of detection without fine-tune<br><br>

### [9.CARS: Huawei proposes a neural network structure search based on evolutionary algorithms and weight sharing. Only one card is needed for half a day on CIFAR-10](https://mp.weixin.qq.com/s/GAL-hbERLp6vS2zB_I9jxg)
<br>
Paper：https://arxiv.org/abs/1909.04977<br>
In order to optimize the problem that the evolutionary algorithm is too long to train the candidate network during the neural network structure search, referring to ENAS and NSGA-III, the paper proposes a continuous evolution architecture search method (CARS) to maximize the use of the learned knowledge, as above The structure and parameters of a round of evolution. First construct a supernet for parameter sharing, generate a subnet from the supernet, and then use the None-dominated sorting strategy to select excellent networks of different sizes. The overall time-consuming is only 0.5 GPU day.<br><br>

### [8.Simplify the complex, new SOTA in the field of weakly supervised target location-Pseudo Supervised Target Location](https://mp.weixin.qq.com/s/6G7BG8DrKZ0Zvi-BUqg78w)<br>

Paper：https://arxiv.org/abs/2002.11359<br>
The paper proposes a pseudo-supervised target localization method (PSOL) to solve the problem of the current weakly supervised target localization method. This method separates the positioning and classification into two independent networks, and then uses Deep descriptor transformation (DDT) to generate a pseudo GT on the training set. After training, the overall effect reaches SOTA. The paper has three main contributions: first, weakly supervised target positioning should be divided into two independent parts: class-agnostic target positioning and target classification, and the PSOL algorithm is proposed; second, despite the deviation of the generated bbox, the paper still believes that they should be directly optimized It does not need class labels, and finally reaches SOTA; Third, on different data sets, the PSOL algorithm does not require fine-tuning and can have good positioning and migration capabilities.<br><br>


### [7.Byte beating: Video 3D human pose estimation based on anatomical perception](https://mp.weixin.qq.com/s/ut8CmEZPc3NMDdlgXfUzGg)<br>

Paper：https://arxiv.org/pdf/2002.10322.pdf<br>
In this work, we propose a new solution for 3D human pose estimation in video. Instead of directly returning to the 3D joint position, we draw inspiration from the human bone anatomy and decompose the task into bone direction prediction and bone length prediction. From these two predictions, we can get the 3D joint position. The motivation for our research is that the length of human bones remains consistent over time. This has driven us to develop effective techniques to utilize the global information of all frames in the video for high-precision bone length prediction. In addition, for the bone direction prediction network, we propose a fully convolutional propagation structure with long hop connections. Essentially, it predicts the direction of different bones hierarchically without using any time-consuming storage unit (such as LSTM). A new joint displacement loss is further introduced to connect the training of the bone length and bone direction prediction network. Finally, we use an implicit attention mechanism to feed back 2D keypoint visibility scores as additional guidance to the model, which significantly alleviates the deep ambiguity in many challenging poses. Our complete model performs better than the previous best results on the Human3.6M and MPI-INF-3dHP datasets. Comprehensive evaluation on these datasets verifies the effectiveness of our model.<br><br>


### [6.Microsoft Asia Research Institute: X-Ray for Deepfake fake face, the new model will change the face to the original shape](https://mp.weixin.qq.com/s/DLxqGFm6IRffPa8A0XBc4w)<br>

Paper：https://arxiv.org/pdf/1912.13458.pdf<br>
Microsoft Asia Research Institute has proposed a method that does not need to know the image data after face replacement, nor does it need to know the face replacement algorithm, can do "X-Ray" on the image, identify whether to change the face, and indicate the face change Border.
The new model Face X-Ray has two major attributes: it can generalize to unknown face-changing algorithms, and it can provide interpretable face-changing boundaries. To obtain such excellent attributes, the trick is hidden in the general process of the face-changing algorithm. As shown below, most face-changing algorithms can be divided into three parts: detection, modification and fusion. Unlike previous research, Face X-Ray hopes to detect errors generated in the third stage.<br><br>

### [5.UDP：Unbiased data processing method in human pose estimation](https://mp.weixin.qq.com/s/J1Y0tSIpfTOZ4J-9PPyhag)<br>

Paper：https://arxiv.org/abs/1911.07524<br>
UDP solves the problem of large statistical errors in the standard encoding and decoding methods in the existing SOTA human pose estimation algorithm. At the same time, the problem of misalignment of results caused by the flip test is solved. And the algorithm is plug and play, which effectively improves the performance of the algorithm without substantially increasing the complexity of the model.<br><br>

### [4.Make synthetic images more real, Shanghai Jiaotong University proposes image harmonization based on domain verification](https://mp.weixin.qq.com/s/JgQ7bgc_bfgWE-PmJMKtOA)<br>

Paper：https://arxiv.org/abs/1911.13239<br>
In the composite image, the foreground and background are shot under different shooting conditions (such as time, season, lighting, weather), so there is a clear mismatch in brightness and color. Image harmonization aims to adjust the foreground in the composite image to make it harmonize with the background. The traditional image harmony method generally transfers color information from the background or other pictures to the foreground, but this cannot guarantee that the adjusted foreground will look real and be in harmony with the background. In recent years, there has been a small amount of work to try to use deep learning for image harmony, but paired synthetic and real images are extremely difficult to obtain. If there is no paired synthetic graph and real graph, the training process of deep learning lacks sufficiently strong supervision information, and the result after the harmony of the synthetic graph is not ground-truth for evaluation. Up to now, there is no public large-scale image harmonization database. We have constructed and published an image harmonization database composed of four sub-databases. And, we put forward the concept of domain verification (domain verification) and tried the image harmony algorithm based on domain verification. **<br><br>

### [3.PolarMask: a new idea for one-stage instance segmentation](https://zhuanlan.zhihu.com/p/84890413)<br>

Paper：https://arxiv.org/abs/1909.13226<br>
PolarMask is based on FCOS and unifies the instance division under the framework of FCN. FCOS is essentially a FCN dense prediction detection framework, which can not lose the anchor based target detection method in performance, allowing the industry to see the potential of the anchor free method. The next problem to be solved is instance segmentation. The biggest contribution of this work is to transform the more complex instance segmentation problem into a task that is as complex as object detection in network design and computational complexity, making modeling instance segmentation simple and efficient.<br><br>

### [2.Huawei GhostNet, surpassing Google MobileNet, has been open source](https://mp.weixin.qq.com/s/Wg_BQpo_3K_fumeelDvUxA)<br>

Paper：https://arxiv.org/abs/1911.11907<br>
The paper provides a brand new Ghost module, which aims to generate more feature maps through cheap operations. Based on a set of original feature maps, the author applies a series of linear transformations to generate many "Ghost feature maps" that can extract the required information from the original features at a small cost. The Ghost module is plug-and-play, and Ghost bottleneck is obtained by stacking Ghost modules, and then a lightweight neural network-GhostNet is built. In the ImageNet classification task, GhostNet's Top-1 accuracy rate is 75.7% with a similar calculation, which is higher than MobileNetV3's 75.2%.<br><br>

### [1.California Institute of Technology Devi Parikh: Multi-task vision and language representation learning](https://mp.weixin.qq.com/s/8CvUT9JvnysIXay7vyY16w)<br>

Paper：https://arxiv.org/abs/1912.02315<br>
Many vision and language studies focus on a small but diverse set of independent tasks and supporting data sets, which are usually studied separately; however, the visual language understanding skills required to successfully complete these tasks overlap greatly . In this work, we study the relationship between vision and language tasks by developing a large-scale, multi-task training mechanism.<br><br>


<br><br>

<a name="103"/>

# 4.To do list<br>
* The CVPR2020 reproduction code is updated in time<br>
* CVPR2020 paper sharing follow-up<br>

<br><br>

<a name="104"/>

# 5.Related links<br>
* [CVPR2019 / 2018/2017 the most complete data download (paper / code, etc.)](https://github.com/extreme-assistant/cvpr2020/blob/master/README.md)<br>
* https://github.com/extreme-assistant/iccv2019<br><br>


# 6.CVPR2020 contributors Wechat Group<br>
In order to allow everyone to communicate better, Jishi has set up a WeChat group of contributors and authors. Welcome to add a small assistant WeChat (cv-mart, note CVPR2020) to the group.
